<table class="fixed-table wrapped"><colgroup><col style="width: 318.0px;" /><col style="width: 186.0px;" /><col style="width: 355.0px;" /><col style="width: 109.0px;" /><col style="width: 116.0px;" /></colgroup><tbody><tr><th>논문 제목</th><th><p>게재지/학회</p></th><th>link</th><th>citations</th><th>저자 소속</th></tr><tr><td colspan="1"><p style="text-align: left;">Dice Loss for Data-imbalanced<span>&nbsp;</span><span class="acl-fixed-case">NLP</span><span>&nbsp;</span>Tasks</p></td><td colspan="1">ACL 2020</td><td colspan="1"><a href="https://aclanthology.org/2020.acl-main.45.pdf">https://aclanthology.org/2020.acl-main.45.pdf</a></td><td colspan="1">267</td><td colspan="1">Shannon.AI</td></tr></tbody></table><p><br /></p><h2><strong>세줄 요약</strong></h2><hr /><p>333</p><h2><strong>Abstract</strong></h2><hr /><p>Tagging 이나 MRC 와 같은 task 들은 data imbalance 문제를 겪고 있다.&nbsp;</p><ul><li>negative example 수가 positive example 수보다 상당히 많고, negative example 의 대부분은 easy-negative example 이다.</li></ul><p>Cross entropy 는 accuracy-oriented 이기 때문에 training 과 test 사이에 불일치(discrepancy)을 발생시킨다.</p><ul><li>training time 에서 각 training instance 는 objective function 에 똑같은 비중으로 기여하지만&nbsp;</li><li>test time 의 결과인 F1 score 는 positive example 을 negative example 보다 더 높은 비중으로 고려한다.&nbsp;</li></ul><p>이 논문에서는 data-imbalanced NLP tasks 를 위해 standard cross-entropy 대신에 <strong>dice loss</strong> 를 사용할 것을 제안한다.</p><ul><li>training time 에서 easy-negative 의 영향을 더 낮추기 위해&nbsp; dynamically adjusted weights 를 사용한다.</li><li>결과적으로 이는 F1 score in test 와 dice loss in training 사이의 갭(gap)을 줄였다.</li><li>또한, POS tagging 과 NER task 에서 SOTA 를 달성 했다.</li></ul><h2><strong>Introduction</strong></h2><hr /><p><ac:image ac:height="150"><ri:attachment ri:filename="image2023-4-24_21-53-12.png" /></ac:image></p><ul><li>NER: 대부분의 class 가 O 이다. (CoNLL03 에서는 5배, OntoNotes5.0 에서는 8배)</li><li>MRC: 일반적인 MRC task 는 staring and ending indices 를 예측하는 방식으로 수행한다. 전체 텍스트 중 보통 2-3 tokens 가 positive 이고 나머지(background)는 negative 이다.&nbsp;</li></ul><p>Data imbalance 는 크게 두 가지 이슈를 낳는다.</p><ol><li>the training-test discrepancy<ul><li>Label 의 밸런스가 맞지 않으면 학습 과정에서 다수의 레이블을 가진 클래스로 강하게 편향되는 경향이 있다. 이는 실제로 training-test 사이의 불일치를 발생시킨다.</li></ul></li><li>the overwhelming effect of easy-negative examples<ul><li>negative examples 이 많다는 것은 easy-negative example 도 많다는 것을 의미한다.&nbsp;</li><li>모델이 positive example 과 hard-negative example 을 구분하는 방법을 충분히 학습하지 못한다.</li></ul></li></ol><p>Cross Entropy 와 MLE(Maximum Likelihood Estimation) 은 이 두 문제를 처리하지 못한다.</p><p><br /></p><p>따라서, dice loss (S&oslash;rensen&ndash;Dice coefficient) 를 제안한다.</p><ul><li>이는 False-Positive 와 False-Negative 를 동등하게 중요시하기 때문에 data imbalance 에 영향을 많이 받지 않는다.</li></ul><p>그러나 dice loss 만으로는 easy-negative 문제를 해결할 수 없다. dice loss 는 본질적으로 soft version of F1 score 이기 때문</p><ul><li>focal loss 에서 영감을 받아, 각 training example 에 (1 - p) 에 비례하는 가중치(weight)를 부여하고 학습이 진행됨에 따라 이 가중치가 동적으로 변화하도록 한다.</li><li>확률 p 가 1에 가까워질수록 학습 도중에 확률이 높은 예제의 비중을 낮춰서 모델이 hard-negative example 에 좀 더 집중하도록 한다.&nbsp;</li></ul><h2><strong>Losses</strong></h2><hr /><ac:structured-macro ac:name="mathjax-block-macro" ac:schema-version="1" ac:macro-id="6c2cac7b-9916-49de-8030-7e824c27f9e7"><ac:plain-text-body><![CDATA[x_i\\in X, y_i = [y_{i0}, y_{i1}], p_i = [p_{i0}, p_{i1}] \\mathbf{where} y_{i0}, y_{i1} \\in {0, 1}, p_{i0},p_{i1}\\in [0,1] and p_{i1}+p_{i0}=1]] ></ac:plain-text-body></ac:structured-macro><h3>Cross Entropy Loss</h3><p>The vanilla cross entropy (CE) loss</p><p><ac:image ac:height="84"><ri:attachment ri:filename="image2023-4-25_13-15-2.png" /></ac:image></p><p>Weighted CE</p><p><ac:image ac:height="80"><ri:attachment ri:filename="image2023-4-25_13-26-28.png" /></ac:image></p><h3>Dice Coefficient and Tversky Index</h3><p><ac:image ac:thumbnail="true" ac:width="200"><ri:attachment ri:filename="image2023-4-25_13-28-6.png" /></ac:image></p><p><ac:image ac:thumbnail="true" ac:width="300"><ri:attachment ri:filename="image2023-4-25_13-28-16.png" /></ac:image></p><p><br /></p><p><br /></p><p><br /></p><ac:structured-macro ac:name="expand" ac:schema-version="1" ac:macro-id="98e7b635-fec1-48b7-b935-af571838cc69"><ac:parameter ac:name="title">Dice Score (Dice coefficient)</ac:parameter><ac:rich-text-body><h5>Vision 분야의 Segmentation task 를 위한 두 평가 지표&nbsp;</h5><p><ac:image ac:border="true" ac:height="150" ac:width="558"><ri:attachment ri:filename="img.png" /></ac:image>Segmentation task</p><p><br /></p><ul><li>IoU (Instersection over Union)<ul><li>두 영역 A, B 가 주어졌을 때 (A 와 B 의 교집합) / (A 와 B 의 합집합)</li><li>두 집합의 차이가 클수록 0에 가깝고 작을수록 1에 가까워진다.&nbsp;</li><li><ac:image ac:thumbnail="true" ac:width="200"><ri:attachment ri:filename="image2023-4-24_21-14-50.png" /></ac:image></li><li><ac:image ac:thumbnail="true" ac:height="150"><ri:attachment ri:filename="image2023-4-24_21-7-26.png" /></ac:image><ac:image ac:height="150"><ri:attachment ri:filename="image2023-4-24_21-9-45.png" /></ac:image></li></ul></li><li>Dice Score<ul><li>GT 와 Prediction, 두 영역의 조화평균을 구하는 평가 지표</li><li><ac:image ac:width="600"><ri:attachment ri:filename="image2023-4-24_21-15-10.png" /></ac:image></li><li><ac:image ac:width="200"><ri:url ri:value="https://blog.kakaocdn.net/dn/dl4wT6/btrExHIjSMg/tpwT1gPL0yFzAPZWNDk8WK/img.png" /></ac:image></li><li>dice score 는 값이 클수록 좋은 것이기 때문에 일반적으로 음의 값을 취해 아래와 같은 공식으로 loss function 을 구한다.<ul><li><em>dice loss = 1 - dice score</em></li></ul></li></ul></li></ul><p><br /></p></ac:rich-text-body></ac:structured-macro><p><br /></p><p><br /></p><p><br /></p>
